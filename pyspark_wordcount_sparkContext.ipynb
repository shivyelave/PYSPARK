{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''\n",
    "\n",
    "\n",
    "\n",
    "    @Author: Shivraj Yelave\n",
    "    @Date: 02-09-24\n",
    "    @Last modified by: Shivraj Yelave\n",
    "    @Last modified time: \n",
    "    @Title: Wordcount using pyspark (sparkContext)\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|text                              |\n",
      "+----------------------------------+\n",
      "|Spark is amazing                  |\n",
      "|PySpark makes data processing easy|\n",
      "|I love Spark                      |\n",
      "+----------------------------------+\n",
      "\n",
      "amazing: 1\n",
      "data: 1\n",
      "easy: 1\n",
      "i: 1\n",
      "is: 1\n",
      "love: 1\n",
      "makes: 1\n",
      "processing: 1\n",
      "pyspark: 1\n",
      "spark: 2\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize SparkConf and SparkContext\n",
    "conf = SparkConf().setAppName(\"WordCount\")\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "# Path to your JSON file\n",
    "json_file_path = \"file:///C:/Users/Admin/Documents/pyspark/PYSPARK/text.json\"\n",
    "\n",
    "# Read JSON file into DataFrame using SQLContext\n",
    "df = sqlContext.read.json(json_file_path)\n",
    "\n",
    "# Drop rows with null values in the 'text' column\n",
    "df = df.select(col('text')).na.drop()\n",
    "\n",
    "# Show the DataFrame to understand its structure\n",
    "df.show(truncate=False)\n",
    "\n",
    "# Extract the 'text' field and split it into words, flatMap to separate the words\n",
    "words_rdd = df.rdd.flatMap(lambda row: row['text'].split(\" \"))\n",
    "\n",
    "# Convert words to lowercase to avoid case sensitivity issues\n",
    "words_rdd = words_rdd.map(lambda word: word.lower())\n",
    "\n",
    "# Map each word to a (word, 1) pair\n",
    "word_pairs_rdd = words_rdd.map(lambda word: (word, 1))\n",
    "\n",
    "# Reduce by key to count occurrences of each word\n",
    "word_count_rdd = word_pairs_rdd.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Collect and print the word counts\n",
    "word_counts = word_count_rdd.collect()\n",
    "\n",
    "# Print the results, sorted for better readability\n",
    "for word, count in sorted(word_counts):\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "# Stop SparkContext\n",
    "sc.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
