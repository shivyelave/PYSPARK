{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''\n",
    "<br><br>\n",
    "\n",
    "@Author: Shivraj Yelave<br>\n",
    "@Date: 03-09-24<br>\n",
    "@Last modified by: Shivraj Yelave<br>\n",
    "@Last modified date: 04-09-24<br>\n",
    "@Title: Covid problems using pyspark <br>\n",
    "<br><br>\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. To find out the death percentage locally and globally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|  Death Percentage|\n",
      "+------------------+\n",
      "|2.2571859631247917|\n",
      "+------------------+\n",
      "\n",
      "+-----------------------+\n",
      "|Global Death Percentage|\n",
      "+-----------------------+\n",
      "|      3.968548255709708|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"DeathPercentage\").getOrCreate()\n",
    "\n",
    "# Read CSV file into DataFrame\n",
    "data = spark.read.csv('file:///C:/Users/Admin/Documents/pyspark/PYSPARK/country_wise_latest.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Calculate the death percentage for each country\n",
    "data = data.withColumn(\"Death Percentage\", col(\"Deaths\") / col(\"Confirmed\") * 100)\n",
    "\n",
    "# Filter the DataFrame for a specific country (e.g., India)\n",
    "local_death_percentage = data.filter(col(\"Country/Region\") == 'India').select(\"Death Percentage\")\n",
    "local_death_percentage.show()\n",
    "\n",
    "# Calculate the global death percentage\n",
    "# Use sum as _sum to avoid conflict with column name\n",
    "global_death_percentage = data.select(\n",
    "    (sum(col(\"Deaths\")) / sum(col(\"Confirmed\")) * 100).alias(\"Global Death Percentage\")\n",
    ")\n",
    "global_death_percentage.show()\n",
    "\n",
    "# Stop session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.To find out the infected population percentage locally and globally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------------+\n",
      "|Country/Region|Local Infected Percent|\n",
      "+--------------+----------------------+\n",
      "|         India|                0.1466|\n",
      "+--------------+----------------------+\n",
      "\n",
      "+-----------------------+\n",
      "|Global Infected Percent|\n",
      "+-----------------------+\n",
      "|                  0.303|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#import modules\n",
    "from pyspark.sql import SparkSession  # Import SparkSession to create a Spark application\n",
    "from pyspark.sql.functions import col, count, sum, avg, max, min, round  # Import functions for DataFrame operations\n",
    "\n",
    "#create session\n",
    "spark = SparkSession.builder.appName(\"question2\").getOrCreate()  # Create a Spark session with the name \"question2\"\n",
    "\n",
    "#load data\n",
    "data = spark.read.csv(\"file:///C:/Users/Admin/Documents/pyspark/PYSPARK/worldometer_data.csv\", header=True, inferSchema=True)  # Load CSV data into a DataFrame with header and inferred schema\n",
    "\n",
    "# Calculate percentage of local infections in India\n",
    "infected_locally = data.filter(col('Country/Region') == 'India').select(['Country/Region', (col('TotalCases') / col('Population') * 100).alias('Local Infected Percent')]).withColumn('Local Infected Percent', round(col('Local Infected Percent'), 4))  # Round the percentage to 4 decimal places\n",
    "\n",
    "infected_locally.show()  # Display the results for local infections in India\n",
    "\n",
    "# Calculate percentage of global infections\n",
    "infected_globally = data.select([(sum(col('TotalCases')) / sum(col('Population')) * 100).alias('Global Infected Percent')]).withColumn('Global Infected Percent', round(col('Global Infected Percent'), 4))  # Round the percentage to 4 decimal places\n",
    "\n",
    "infected_globally.show()  # Display the results for global infections\n",
    "\n",
    "spark.stop()  # Stop the Spark session to release resources\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. To find out the countries with the highest infection rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------+\n",
      "|    Countries|Infected Percent|\n",
      "+-------------+----------------+\n",
      "|        Qatar|           3.992|\n",
      "|French Guiana|           2.715|\n",
      "|      Bahrain|           2.513|\n",
      "|   San Marino|            2.06|\n",
      "|        Chile|           1.916|\n",
      "|       Panama|           1.653|\n",
      "|       Kuwait|           1.638|\n",
      "|         Oman|           1.577|\n",
      "|          USA|           1.519|\n",
      "| Vatican City|           1.498|\n",
      "+-------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession  # Import SparkSession to create a Spark application\n",
    "\n",
    "spark = SparkSession.builder.appName(\"question 3\").getOrCreate()  # Create a Spark session with the name \"question 3\"\n",
    "\n",
    "data = spark.read.csv(\"file:///C:/Users/Admin/Documents/pyspark/PYSPARK/worldometer_data.csv\", header=True, inferSchema=True)  # Load CSV data into a DataFrame with header and inferred schema\n",
    "\n",
    "# Select relevant columns and calculate the percentage of infected cases\n",
    "required_data = data.select([  # Select columns from the DataFrame\n",
    "    col('Country/Region').alias(\"Countries\"),  # Rename 'Country/Region' column to 'Countries'\n",
    "    (col('TotalCases') / col('Population') * 100).alias('Infected Percent')  # Calculate infected percentage and alias it as 'Infected Percent'\n",
    "]).orderBy(col('Infected Percent'), ascending=False)  # Order the DataFrame by 'Infected Percent' in descending order\n",
    "\n",
    "required_data = required_data.withColumn('Infected Percent', round(col('Infected Percent'), 3))  # Round the 'Infected Percent' to 3 decimal places\n",
    "\n",
    "required_data = required_data.limit(10)  # Limit the result to the top 10 rows\n",
    "\n",
    "required_data.show()  # Display the DataFrame showing the top 10 countries with the highest infection percentages\n",
    "\n",
    "spark.stop()  # Stop the Spark session to release resources\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. To find out the countries and continents with the highest death counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|Countries|Deaths|\n",
      "+---------+------+\n",
      "|      USA|162804|\n",
      "|   Brazil| 98644|\n",
      "|   Mexico| 50517|\n",
      "|       UK| 46413|\n",
      "|    India| 41638|\n",
      "|    Italy| 35187|\n",
      "|   France| 30312|\n",
      "|    Spain| 28500|\n",
      "|     Peru| 20424|\n",
      "|     Iran| 17976|\n",
      "+---------+------+\n",
      "\n",
      "+--------------------+------+\n",
      "|          Continents|Deaths|\n",
      "+--------------------+------+\n",
      "|            Americas|384637|\n",
      "|              Europe|215564|\n",
      "|      South-EastAsia| 50624|\n",
      "|EasternMediterranean| 42376|\n",
      "|              Africa| 15538|\n",
      "|      WesternPacific|  3975|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName('question 4').getOrCreate()\n",
    "\n",
    "# Load the CSV data into a DataFrame\n",
    "data = spark.read.csv('file:///C:/Users/Admin/Documents/pyspark/PYSPARK/worldometer_data.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Select the top 10 countries with the highest number of deaths\n",
    "required_data = data.select([col('Country/Region').alias('Countries'), col('TotalDeaths').alias('Deaths')]) \\\n",
    "                    .orderBy(col('Deaths'), ascending=False) \\\n",
    "                    .limit(10)\n",
    "\n",
    "# Show the result for countries with the most deaths\n",
    "required_data.show()\n",
    "\n",
    "# Group by WHO Region (Continents) and sum the total deaths, then order by deaths in descending order\n",
    "required_data2 = data.groupBy(col('WHO Region').alias('Continents')) \\\n",
    "                    .agg(sum(col('TotalDeaths')).alias('Deaths')) \\\n",
    "                    .orderBy(col('Deaths'), ascending=False) \\\n",
    "                    .limit(10)\n",
    "\n",
    "# Show the result for continents with the most deaths\n",
    "required_data2.na.drop().show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Average number of deaths by day (Continents and Countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+\n",
      "|      Date|          WHO Region|          Avg Deaths|\n",
      "+----------+--------------------+--------------------+\n",
      "|01-02-2020|              Europe|                 0.0|\n",
      "|01-02-2020|Eastern Mediterra...|                 0.0|\n",
      "|01-02-2020|            Americas|                 0.0|\n",
      "|01-02-2020|     Western Pacific|   4.709090909090909|\n",
      "|01-02-2020|              Africa|                 0.0|\n",
      "|01-02-2020|     South-East Asia|                 0.0|\n",
      "|01-03-2020|     Western Pacific|   52.69090909090909|\n",
      "|01-03-2020|            Americas|0.021739130434782608|\n",
      "|01-03-2020|Eastern Mediterra...|  2.4545454545454546|\n",
      "|01-03-2020|              Africa|                 0.0|\n",
      "|01-03-2020|              Europe|                0.45|\n",
      "|01-03-2020|     South-East Asia|                 0.1|\n",
      "|01-04-2020|              Europe|             438.725|\n",
      "|01-04-2020|     South-East Asia|                23.7|\n",
      "|01-04-2020|              Africa|              2.6875|\n",
      "|01-04-2020|     Western Pacific|    67.7090909090909|\n",
      "|01-04-2020|Eastern Mediterra...|  149.27272727272728|\n",
      "|01-04-2020|            Americas|   164.2826086956522|\n",
      "|01-05-2020|            Americas|  1779.2173913043478|\n",
      "|01-05-2020|     South-East Asia|               226.1|\n",
      "+----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------+--------------+----------+\n",
      "|      Date|Country/Region|Avg Deaths|\n",
      "+----------+--------------+----------+\n",
      "|01-02-2020|      Dominica|       0.0|\n",
      "|01-02-2020|  Sierra Leone|       0.0|\n",
      "|01-02-2020|       Comoros|       0.0|\n",
      "|01-02-2020|   New Zealand|       0.0|\n",
      "|01-02-2020|       Lesotho|       0.0|\n",
      "|01-02-2020|      Malaysia|       0.0|\n",
      "|01-02-2020|        Rwanda|       0.0|\n",
      "|01-02-2020|      Barbados|       0.0|\n",
      "|01-02-2020|         Japan|       0.0|\n",
      "|01-02-2020|     Australia|       0.0|\n",
      "|01-02-2020|       Belgium|       0.0|\n",
      "|01-02-2020|       Algeria|       0.0|\n",
      "|01-02-2020|    Azerbaijan|       0.0|\n",
      "|01-02-2020|       Bahamas|       0.0|\n",
      "|01-02-2020|       Senegal|       0.0|\n",
      "|01-02-2020|    Tajikistan|       0.0|\n",
      "|01-02-2020|      Djibouti|       0.0|\n",
      "|01-02-2020|          Chad|       0.0|\n",
      "|01-02-2020|   Timor-Leste|       0.0|\n",
      "|01-02-2020|      Suriname|       0.0|\n",
      "+----------+--------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, avg\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName('question 5').getOrCreate()\n",
    "\n",
    "# Load the CSV data into a DataFrame\n",
    "data = spark.read.csv('file:///C:/Users/Admin/Documents/pyspark/PYSPARK/covid_19_clean_complete.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Group the data by Date and 'WHO Region' (or Continent) and calculate the average number of deaths by day\n",
    "avg_deaths_by_continent = data.groupBy(col('Date'), col('WHO Region')) \\\n",
    "    .agg(avg(col('Deaths')).alias('Avg Deaths')) \\\n",
    "    .orderBy(col('Date'), ascending=True)\n",
    "\n",
    "avg_deaths_by_continent.show()\n",
    "\n",
    "# Group the data by Date and 'Country/Region' and calculate the average number of deaths by day\n",
    "avg_deaths_by_country = data.groupBy(col('Date'), col('Country/Region')) \\\n",
    "    .agg(avg(col('Deaths')).alias('Avg Deaths')) \\\n",
    "    .orderBy(col('Date'), ascending=True)\n",
    "\n",
    "avg_deaths_by_country.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Average of cases divided by the number of population of each country (TOP 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------------+\n",
      "|Country/Region|Local Infected Percent|\n",
      "+--------------+----------------------+\n",
      "|         India|                0.1466|\n",
      "+--------------+----------------------+\n",
      "\n",
      "+-----------------------+\n",
      "|Global Infected Percent|\n",
      "+-----------------------+\n",
      "|                  0.303|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#import modules\n",
    "from pyspark.sql import SparkSession  # Import SparkSession to create a Spark application\n",
    "from pyspark.sql.functions import col, count, sum, avg, max, min, round  # Import functions for DataFrame operations\n",
    "\n",
    "#create session\n",
    "spark = SparkSession.builder.appName(\"question2\").getOrCreate()  # Create a Spark session with the name \"question2\"\n",
    "\n",
    "#load data\n",
    "data = spark.read.csv(\"file:///C:/Users/Admin/Documents/pyspark/PYSPARK/worldometer_data.csv\", header=True, inferSchema=True)  # Load CSV data into a DataFrame with header and inferred schema\n",
    "\n",
    "# Calculate percentage of local infections in India\n",
    "infected_locally = data.filter(col('Country/Region') == 'India').select(['Country/Region', (col('TotalCases') / col('Population') * 100).alias('Local Infected Percent')]).withColumn('Local Infected Percent', round(col('Local Infected Percent'), 4))  # Round the percentage to 4 decimal places\n",
    "\n",
    "infected_locally.show()  # Display the results for local infections in India\n",
    "\n",
    "# Calculate percentage of global infections\n",
    "infected_globally = data.select([(sum(col('TotalCases')) / sum(col('Population')) * 100).alias('Global Infected Percent')]).withColumn('Global Infected Percent', round(col('Global Infected Percent'), 4))  # Round the percentage to 4 decimal places\n",
    "\n",
    "infected_globally.show()  # Display the results for global infections\n",
    "\n",
    "spark.stop()  # Stop the Spark session to release resources\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
